
==> Audit <==
|-----------|--------------------------------------------------|----------|--------|---------|---------------------|---------------------|
|  Command  |                       Args                       | Profile  |  User  | Version |     Start Time      |      End Time       |
|-----------|--------------------------------------------------|----------|--------|---------|---------------------|---------------------|
| start     | --extra-config                                   | minikube | atviks | v1.34.0 | 12 Oct 24 23:07 JST |                     |
|           | apiserver.cors-allowed-origins=[http://boot.dev] |          |        |         |                     |                     |
| start     | --extra-config                                   | minikube | atviks | v1.34.0 | 13 Oct 24 21:23 JST |                     |
|           | apiserver.cors-allowed-origins=[http://boot.dev] |          |        |         |                     |                     |
| start     | --extra-config                                   | minikube | atviks | v1.34.0 | 13 Oct 24 21:30 JST | 13 Oct 24 21:34 JST |
|           | apiserver.cors-allowed-origins=[http://boot.dev] |          |        |         |                     |                     |
| dashboard | --port=63840                                     | minikube | atviks | v1.34.0 | 13 Oct 24 21:38 JST |                     |
| dashboard | --url --port=63840                               | minikube | atviks | v1.34.0 | 13 Oct 24 21:57 JST |                     |
| stop      |                                                  | minikube | atviks | v1.34.0 | 13 Oct 24 21:58 JST | 13 Oct 24 21:58 JST |
| start     | --extra-config                                   | minikube | atviks | v1.34.0 | 13 Oct 24 21:58 JST | 13 Oct 24 21:59 JST |
|           | apiserver.cors-allowed-origins=[http://boot.dev] |          |        |         |                     |                     |
| dashboard | --url --port=63840                               | minikube | atviks | v1.34.0 | 13 Oct 24 21:59 JST |                     |
|-----------|--------------------------------------------------|----------|--------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/10/13 21:58:47
Running on machine: LAPTOP-BQBA7DGO
Binary: Built with gc go1.22.5 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1013 21:58:47.895949   36043 out.go:345] Setting OutFile to fd 1 ...
I1013 21:58:47.896206   36043 out.go:397] isatty.IsTerminal(1) = true
I1013 21:58:47.896211   36043 out.go:358] Setting ErrFile to fd 2...
I1013 21:58:47.896216   36043 out.go:397] isatty.IsTerminal(2) = true
I1013 21:58:47.896526   36043 root.go:338] Updating PATH: /home/atviks/.minikube/bin
W1013 21:58:47.896763   36043 root.go:314] Error reading config file at /home/atviks/.minikube/config/config.json: open /home/atviks/.minikube/config/config.json: no such file or directory
I1013 21:58:47.897210   36043 out.go:352] Setting JSON to false
I1013 21:58:47.900319   36043 start.go:129] hostinfo: {"hostname":"LAPTOP-BQBA7DGO","uptime":4022,"bootTime":1728820305,"procs":70,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.15.153.1-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"25de8f9b-63c3-40aa-9b5f-4a3569b3abc5"}
I1013 21:58:47.900362   36043 start.go:139] virtualization:  guest
I1013 21:58:47.902553   36043 out.go:177] 😄  minikube v1.34.0 on Ubuntu 22.04 (amd64)
I1013 21:58:47.904365   36043 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1013 21:58:47.904435   36043 notify.go:220] Checking for updates...
I1013 21:58:47.905281   36043 driver.go:394] Setting default libvirt URI to qemu:///system
I1013 21:58:47.935476   36043 docker.go:123] docker version: linux-27.1.1:Docker Desktop  ()
I1013 21:58:47.935663   36043 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1013 21:58:48.684239   36043 info.go:266] docker info: {ID:2b791d8c-68ac-41a1-ad07-0b1c64423d31 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:60 OomKillDisable:true NGoroutines:80 SystemTime:2024-10-13 12:58:48.673726886 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:8204419072 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///var/run/docker-cli.sock] ExperimentalBuild:false ServerVersion:27.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2bf793ef6dc9a18e00cb12efb64355c2c9d5eb41 Expected:2bf793ef6dc9a18e00cb12efb64355c2c9d5eb41} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.16.1-desktop.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.1-desktop.1] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.34] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.25] map[Name:feedback Path:/usr/local/lib/docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.11.0]] Warnings:<nil>}}
I1013 21:58:48.684387   36043 docker.go:318] overlay module found
I1013 21:58:48.685420   36043 out.go:177] ✨  Using the docker driver based on existing profile
I1013 21:58:48.686196   36043 start.go:297] selected driver: docker
I1013 21:58:48.686201   36043 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:apiserver Key:cors-allowed-origins Value:[http://boot.dev]}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.220.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/atviks:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1013 21:58:48.686259   36043 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1013 21:58:48.686626   36043 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1013 21:58:48.778343   36043 info.go:266] docker info: {ID:2b791d8c-68ac-41a1-ad07-0b1c64423d31 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:60 OomKillDisable:true NGoroutines:80 SystemTime:2024-10-13 12:58:48.76889197 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:8204419072 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///var/run/docker-cli.sock] ExperimentalBuild:false ServerVersion:27.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2bf793ef6dc9a18e00cb12efb64355c2c9d5eb41 Expected:2bf793ef6dc9a18e00cb12efb64355c2c9d5eb41} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/local/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.16.1-desktop.1] map[Name:compose Path:/usr/local/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.1-desktop.1] map[Name:debug Path:/usr/local/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.34] map[Name:dev Path:/usr/local/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/local/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.25] map[Name:feedback Path:/usr/local/lib/docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:/usr/local/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:/usr/local/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/local/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.11.0]] Warnings:<nil>}}
I1013 21:58:48.780175   36043 cni.go:84] Creating CNI manager for ""
I1013 21:58:48.780230   36043 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1013 21:58:48.780280   36043 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:apiserver Key:cors-allowed-origins Value:[http://boot.dev]}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.220.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/atviks:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1013 21:58:48.781271   36043 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I1013 21:58:48.782186   36043 cache.go:121] Beginning downloading kic base image for docker with docker
I1013 21:58:48.782944   36043 out.go:177] 🚜  Pulling base image v0.0.45 ...
I1013 21:58:48.783657   36043 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1013 21:58:48.783684   36043 preload.go:146] Found local preload: /home/atviks/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I1013 21:58:48.783692   36043 cache.go:56] Caching tarball of preloaded images
I1013 21:58:48.783817   36043 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local docker daemon
I1013 21:58:48.783931   36043 preload.go:172] Found /home/atviks/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1013 21:58:48.783955   36043 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I1013 21:58:48.784027   36043 profile.go:143] Saving config to /home/atviks/.minikube/profiles/minikube/config.json ...
W1013 21:58:48.824491   36043 image.go:95] image gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 is of wrong architecture
I1013 21:58:48.824500   36043 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I1013 21:58:48.824646   36043 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory
I1013 21:58:48.824656   36043 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory, skipping pull
I1013 21:58:48.824659   36043 image.go:135] gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 exists in cache, skipping pull
I1013 21:58:48.824685   36043 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 as a tarball
I1013 21:58:48.824689   36043 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from local cache
I1013 21:58:49.400174   36043 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from cached tarball
I1013 21:58:49.400248   36043 cache.go:194] Successfully downloaded all kic artifacts
I1013 21:58:49.400369   36043 start.go:360] acquireMachinesLock for minikube: {Name:mkc86731d7939ed869c13b631f86e222d3bbe756 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1013 21:58:49.400451   36043 start.go:364] duration metric: took 64.167µs to acquireMachinesLock for "minikube"
I1013 21:58:49.400558   36043 start.go:96] Skipping create...Using existing machine configuration
I1013 21:58:49.400583   36043 fix.go:54] fixHost starting: 
I1013 21:58:49.400834   36043 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1013 21:58:49.422252   36043 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1013 21:58:49.422323   36043 fix.go:138] unexpected machine state, will restart: <nil>
I1013 21:58:49.423683   36043 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I1013 21:58:49.424778   36043 cli_runner.go:164] Run: docker start minikube
I1013 21:58:49.827466   36043 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1013 21:58:49.855848   36043 kic.go:430] container "minikube" state is running.
I1013 21:58:49.857209   36043 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1013 21:58:49.884733   36043 profile.go:143] Saving config to /home/atviks/.minikube/profiles/minikube/config.json ...
I1013 21:58:49.885206   36043 machine.go:93] provisionDockerMachine start ...
I1013 21:58:49.885289   36043 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1013 21:58:49.906495   36043 main.go:141] libmachine: Using SSH client type: native
I1013 21:58:49.906807   36043 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 51373 <nil> <nil>}
I1013 21:58:49.906812   36043 main.go:141] libmachine: About to run SSH command:
hostname
I1013 21:58:49.907366   36043 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1013 21:58:53.023535   36043 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1013 21:58:53.023586   36043 ubuntu.go:169] provisioning hostname "minikube"
I1013 21:58:53.023665   36043 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1013 21:58:53.042767   36043 main.go:141] libmachine: Using SSH client type: native
I1013 21:58:53.042894   36043 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 51373 <nil> <nil>}
I1013 21:58:53.042899   36043 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1013 21:58:53.170299   36043 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1013 21:58:53.170387   36043 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1013 21:58:53.193356   36043 main.go:141] libmachine: Using SSH client type: native
I1013 21:58:53.193525   36043 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 51373 <nil> <nil>}
I1013 21:58:53.193536   36043 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1013 21:58:53.323188   36043 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1013 21:58:53.323204   36043 ubuntu.go:175] set auth options {CertDir:/home/atviks/.minikube CaCertPath:/home/atviks/.minikube/certs/ca.pem CaPrivateKeyPath:/home/atviks/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/atviks/.minikube/machines/server.pem ServerKeyPath:/home/atviks/.minikube/machines/server-key.pem ClientKeyPath:/home/atviks/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/atviks/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/atviks/.minikube}
I1013 21:58:53.323217   36043 ubuntu.go:177] setting up certificates
I1013 21:58:53.323223   36043 provision.go:84] configureAuth start
I1013 21:58:53.323290   36043 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1013 21:58:53.351282   36043 provision.go:143] copyHostCerts
I1013 21:58:53.351338   36043 exec_runner.go:144] found /home/atviks/.minikube/ca.pem, removing ...
I1013 21:58:53.351541   36043 exec_runner.go:203] rm: /home/atviks/.minikube/ca.pem
I1013 21:58:53.351595   36043 exec_runner.go:151] cp: /home/atviks/.minikube/certs/ca.pem --> /home/atviks/.minikube/ca.pem (1078 bytes)
I1013 21:58:53.352789   36043 exec_runner.go:144] found /home/atviks/.minikube/cert.pem, removing ...
I1013 21:58:53.352797   36043 exec_runner.go:203] rm: /home/atviks/.minikube/cert.pem
I1013 21:58:53.352828   36043 exec_runner.go:151] cp: /home/atviks/.minikube/certs/cert.pem --> /home/atviks/.minikube/cert.pem (1123 bytes)
I1013 21:58:53.353009   36043 exec_runner.go:144] found /home/atviks/.minikube/key.pem, removing ...
I1013 21:58:53.353012   36043 exec_runner.go:203] rm: /home/atviks/.minikube/key.pem
I1013 21:58:53.353029   36043 exec_runner.go:151] cp: /home/atviks/.minikube/certs/key.pem --> /home/atviks/.minikube/key.pem (1675 bytes)
I1013 21:58:53.353206   36043 provision.go:117] generating server cert: /home/atviks/.minikube/machines/server.pem ca-key=/home/atviks/.minikube/certs/ca.pem private-key=/home/atviks/.minikube/certs/ca-key.pem org=atviks.minikube san=[127.0.0.1 192.168.220.2 localhost minikube]
I1013 21:58:53.584043   36043 provision.go:177] copyRemoteCerts
I1013 21:58:53.584112   36043 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1013 21:58:53.584155   36043 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1013 21:58:53.606161   36043 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51373 SSHKeyPath:/home/atviks/.minikube/machines/minikube/id_rsa Username:docker}
I1013 21:58:53.695692   36043 ssh_runner.go:362] scp /home/atviks/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I1013 21:58:53.710493   36043 ssh_runner.go:362] scp /home/atviks/.minikube/machines/server.pem --> /etc/docker/server.pem (1180 bytes)
I1013 21:58:53.725713   36043 ssh_runner.go:362] scp /home/atviks/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1013 21:58:53.740896   36043 provision.go:87] duration metric: took 417.662866ms to configureAuth
I1013 21:58:53.740914   36043 ubuntu.go:193] setting minikube options for container-runtime
I1013 21:58:53.741029   36043 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1013 21:58:53.741086   36043 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1013 21:58:53.759923   36043 main.go:141] libmachine: Using SSH client type: native
I1013 21:58:53.760049   36043 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 51373 <nil> <nil>}
I1013 21:58:53.760053   36043 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1013 21:58:53.883591   36043 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1013 21:58:53.883604   36043 ubuntu.go:71] root file system type: overlay
I1013 21:58:53.883676   36043 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1013 21:58:53.883754   36043 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1013 21:58:53.904560   36043 main.go:141] libmachine: Using SSH client type: native
I1013 21:58:53.904675   36043 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 51373 <nil> <nil>}
I1013 21:58:53.904744   36043 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1013 21:58:54.031109   36043 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1013 21:58:54.031209   36043 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1013 21:58:54.055589   36043 main.go:141] libmachine: Using SSH client type: native
I1013 21:58:54.055711   36043 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82f9c0] 0x832720 <nil>  [] 0s} 127.0.0.1 51373 <nil> <nil>}
I1013 21:58:54.055719   36043 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1013 21:58:54.176586   36043 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1013 21:58:54.176605   36043 machine.go:96] duration metric: took 4.291388876s to provisionDockerMachine
I1013 21:58:54.176639   36043 start.go:293] postStartSetup for "minikube" (driver="docker")
I1013 21:58:54.176649   36043 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1013 21:58:54.176739   36043 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1013 21:58:54.176800   36043 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1013 21:58:54.199668   36043 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51373 SSHKeyPath:/home/atviks/.minikube/machines/minikube/id_rsa Username:docker}
I1013 21:58:54.296647   36043 ssh_runner.go:195] Run: cat /etc/os-release
I1013 21:58:54.299312   36043 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1013 21:58:54.299338   36043 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1013 21:58:54.299344   36043 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1013 21:58:54.299352   36043 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I1013 21:58:54.299392   36043 filesync.go:126] Scanning /home/atviks/.minikube/addons for local assets ...
I1013 21:58:54.299675   36043 filesync.go:126] Scanning /home/atviks/.minikube/files for local assets ...
I1013 21:58:54.299879   36043 start.go:296] duration metric: took 123.234001ms for postStartSetup
I1013 21:58:54.299954   36043 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1013 21:58:54.300001   36043 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1013 21:58:54.319723   36043 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51373 SSHKeyPath:/home/atviks/.minikube/machines/minikube/id_rsa Username:docker}
I1013 21:58:54.403916   36043 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1013 21:58:54.407246   36043 fix.go:56] duration metric: took 5.006646927s for fixHost
I1013 21:58:54.407280   36043 start.go:83] releasing machines lock for "minikube", held for 5.006747691s
I1013 21:58:54.407446   36043 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1013 21:58:54.427816   36043 ssh_runner.go:195] Run: cat /version.json
I1013 21:58:54.427877   36043 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1013 21:58:54.427877   36043 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1013 21:58:54.427957   36043 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1013 21:58:54.450015   36043 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51373 SSHKeyPath:/home/atviks/.minikube/machines/minikube/id_rsa Username:docker}
I1013 21:58:54.450990   36043 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51373 SSHKeyPath:/home/atviks/.minikube/machines/minikube/id_rsa Username:docker}
I1013 21:58:54.651943   36043 ssh_runner.go:195] Run: systemctl --version
I1013 21:58:54.655207   36043 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1013 21:58:54.658519   36043 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1013 21:58:54.671830   36043 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1013 21:58:54.671911   36043 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1013 21:58:54.678171   36043 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1013 21:58:54.678186   36043 start.go:495] detecting cgroup driver to use...
I1013 21:58:54.678208   36043 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1013 21:58:54.678398   36043 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1013 21:58:54.689447   36043 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I1013 21:58:54.697306   36043 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1013 21:58:54.704469   36043 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1013 21:58:54.704532   36043 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1013 21:58:54.712157   36043 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1013 21:58:54.720665   36043 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1013 21:58:54.730435   36043 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1013 21:58:54.737774   36043 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1013 21:58:54.744426   36043 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1013 21:58:54.751800   36043 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1013 21:58:54.759145   36043 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1013 21:58:54.776410   36043 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1013 21:58:54.783029   36043 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1013 21:58:54.788879   36043 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1013 21:58:54.884255   36043 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1013 21:58:54.989744   36043 start.go:495] detecting cgroup driver to use...
I1013 21:58:54.989785   36043 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1013 21:58:54.989893   36043 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1013 21:58:54.997750   36043 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1013 21:58:54.997815   36043 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1013 21:58:55.006410   36043 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1013 21:58:55.017509   36043 ssh_runner.go:195] Run: which cri-dockerd
I1013 21:58:55.020614   36043 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1013 21:58:55.026193   36043 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I1013 21:58:55.038672   36043 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1013 21:58:55.228499   36043 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1013 21:58:55.410724   36043 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I1013 21:58:55.410812   36043 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1013 21:58:55.422665   36043 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1013 21:58:55.521593   36043 ssh_runner.go:195] Run: sudo systemctl restart docker
I1013 21:58:56.083726   36043 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1013 21:58:56.092562   36043 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I1013 21:58:56.101898   36043 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1013 21:58:56.111669   36043 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1013 21:58:56.201468   36043 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1013 21:58:56.289875   36043 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1013 21:58:56.383697   36043 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1013 21:58:56.395795   36043 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1013 21:58:56.406830   36043 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1013 21:58:56.495182   36043 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1013 21:58:56.555657   36043 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1013 21:58:56.555733   36043 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1013 21:58:56.559232   36043 start.go:563] Will wait 60s for crictl version
I1013 21:58:56.559292   36043 ssh_runner.go:195] Run: which crictl
I1013 21:58:56.562538   36043 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1013 21:58:56.590339   36043 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I1013 21:58:56.590419   36043 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1013 21:58:56.613172   36043 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1013 21:58:56.635885   36043 out.go:235] 🐳  Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...
I1013 21:58:56.636402   36043 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1013 21:58:56.659536   36043 ssh_runner.go:195] Run: grep 192.168.220.1	host.minikube.internal$ /etc/hosts
I1013 21:58:56.663607   36043 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.220.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1013 21:58:56.671952   36043 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1013 21:58:56.698398   36043 out.go:177]     ▪ apiserver.cors-allowed-origins=[http://boot.dev]
I1013 21:58:56.699385   36043 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:apiserver Key:cors-allowed-origins Value:[http://boot.dev]}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.220.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/atviks:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1013 21:58:56.699519   36043 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1013 21:58:56.699645   36043 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1013 21:58:56.718827   36043 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1013 21:58:56.718840   36043 docker.go:615] Images already preloaded, skipping extraction
I1013 21:58:56.718937   36043 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1013 21:58:56.737804   36043 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1013 21:58:56.737846   36043 cache_images.go:84] Images are preloaded, skipping loading
I1013 21:58:56.737857   36043 kubeadm.go:934] updating node { 192.168.220.2 8443 v1.31.0 docker true true} ...
I1013 21:58:56.737971   36043 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.220.2

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:apiserver Key:cors-allowed-origins Value:[http://boot.dev]}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1013 21:58:56.738064   36043 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1013 21:58:56.776139   36043 cni.go:84] Creating CNI manager for ""
I1013 21:58:56.776151   36043 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1013 21:58:56.776594   36043 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1013 21:58:56.776628   36043 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.220.2 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[cors-allowed-origins:[http://boot.dev] enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.220.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.220.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1013 21:58:56.776723   36043 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.220.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.220.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.220.2"]
  extraArgs:
    cors-allowed-origins: "[http://boot.dev]"
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1013 21:58:56.776805   36043 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I1013 21:58:56.783540   36043 binaries.go:44] Found k8s binaries, skipping transfer
I1013 21:58:56.783656   36043 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1013 21:58:56.789569   36043 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (308 bytes)
I1013 21:58:56.800994   36043 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1013 21:58:56.813009   36043 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2199 bytes)
I1013 21:58:56.825962   36043 ssh_runner.go:195] Run: grep 192.168.220.2	control-plane.minikube.internal$ /etc/hosts
I1013 21:58:56.828829   36043 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.220.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1013 21:58:56.835963   36043 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1013 21:58:56.909956   36043 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1013 21:58:56.919840   36043 certs.go:68] Setting up /home/atviks/.minikube/profiles/minikube for IP: 192.168.220.2
I1013 21:58:56.919872   36043 certs.go:194] generating shared ca certs ...
I1013 21:58:56.919884   36043 certs.go:226] acquiring lock for ca certs: {Name:mk82e0c43a06e46e059e9aa7665374872b2d550b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1013 21:58:56.920279   36043 certs.go:235] skipping valid "minikubeCA" ca cert: /home/atviks/.minikube/ca.key
I1013 21:58:56.920530   36043 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/atviks/.minikube/proxy-client-ca.key
I1013 21:58:56.920542   36043 certs.go:256] generating profile certs ...
I1013 21:58:56.920664   36043 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/atviks/.minikube/profiles/minikube/client.key
I1013 21:58:56.920905   36043 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/atviks/.minikube/profiles/minikube/apiserver.key.2f62a64d
I1013 21:58:56.921136   36043 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/atviks/.minikube/profiles/minikube/proxy-client.key
I1013 21:58:56.921473   36043 certs.go:484] found cert: /home/atviks/.minikube/certs/ca-key.pem (1679 bytes)
I1013 21:58:56.921495   36043 certs.go:484] found cert: /home/atviks/.minikube/certs/ca.pem (1078 bytes)
I1013 21:58:56.921508   36043 certs.go:484] found cert: /home/atviks/.minikube/certs/cert.pem (1123 bytes)
I1013 21:58:56.921518   36043 certs.go:484] found cert: /home/atviks/.minikube/certs/key.pem (1675 bytes)
I1013 21:58:56.922534   36043 ssh_runner.go:362] scp /home/atviks/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1013 21:58:56.939242   36043 ssh_runner.go:362] scp /home/atviks/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1013 21:58:56.956953   36043 ssh_runner.go:362] scp /home/atviks/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1013 21:58:56.974491   36043 ssh_runner.go:362] scp /home/atviks/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1013 21:58:56.992286   36043 ssh_runner.go:362] scp /home/atviks/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1013 21:58:57.066586   36043 ssh_runner.go:362] scp /home/atviks/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1013 21:58:57.083422   36043 ssh_runner.go:362] scp /home/atviks/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1013 21:58:57.099922   36043 ssh_runner.go:362] scp /home/atviks/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I1013 21:58:57.117162   36043 ssh_runner.go:362] scp /home/atviks/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1013 21:58:57.163795   36043 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1013 21:58:57.175367   36043 ssh_runner.go:195] Run: openssl version
I1013 21:58:57.178906   36043 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1013 21:58:57.185590   36043 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1013 21:58:57.187919   36043 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Oct 13 12:34 /usr/share/ca-certificates/minikubeCA.pem
I1013 21:58:57.187960   36043 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1013 21:58:57.192281   36043 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1013 21:58:57.198487   36043 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1013 21:58:57.202450   36043 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1013 21:58:57.208340   36043 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1013 21:58:57.213361   36043 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1013 21:58:57.218522   36043 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1013 21:58:57.225032   36043 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1013 21:58:57.231924   36043 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1013 21:58:57.237692   36043 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[{Component:apiserver Key:cors-allowed-origins Value:[http://boot.dev]}] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.220.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/atviks:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1013 21:58:57.237816   36043 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1013 21:58:57.257104   36043 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1013 21:58:57.264350   36043 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I1013 21:58:57.264508   36043 kubeadm.go:593] restartPrimaryControlPlane start ...
I1013 21:58:57.264585   36043 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1013 21:58:57.270910   36043 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1013 21:58:57.270989   36043 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1013 21:58:57.293315   36043 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in /home/atviks/.kube/config
I1013 21:58:57.293718   36043 kubeconfig.go:62] /home/atviks/.kube/config needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
I1013 21:58:57.294388   36043 lock.go:35] WriteFile acquiring /home/atviks/.kube/config: {Name:mk92934aa2511661ae737a7d44815cefdc0e7426 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1013 21:58:57.295222   36043 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1013 21:58:57.303524   36043 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I1013 21:58:57.303854   36043 kubeadm.go:597] duration metric: took 39.336548ms to restartPrimaryControlPlane
I1013 21:58:57.303882   36043 kubeadm.go:394] duration metric: took 66.226536ms to StartCluster
I1013 21:58:57.303907   36043 settings.go:142] acquiring lock: {Name:mk07d1e2f7118b010be99de9867963b6ad618a05 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1013 21:58:57.304000   36043 settings.go:150] Updating kubeconfig:  /home/atviks/.kube/config
I1013 21:58:57.304635   36043 lock.go:35] WriteFile acquiring /home/atviks/.kube/config: {Name:mk92934aa2511661ae737a7d44815cefdc0e7426 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1013 21:58:57.304918   36043 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.220.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1013 21:58:57.305060   36043 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1013 21:58:57.304966   36043 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1013 21:58:57.305091   36043 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1013 21:58:57.305123   36043 addons.go:69] Setting dashboard=true in profile "minikube"
I1013 21:58:57.305132   36043 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W1013 21:58:57.305136   36043 addons.go:243] addon storage-provisioner should already be in state true
I1013 21:58:57.305145   36043 addons.go:234] Setting addon dashboard=true in "minikube"
W1013 21:58:57.305149   36043 addons.go:243] addon dashboard should already be in state true
I1013 21:58:57.305169   36043 host.go:66] Checking if "minikube" exists ...
I1013 21:58:57.305171   36043 host.go:66] Checking if "minikube" exists ...
I1013 21:58:57.305182   36043 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1013 21:58:57.305198   36043 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1013 21:58:57.305387   36043 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1013 21:58:57.305416   36043 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1013 21:58:57.305434   36043 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1013 21:58:57.307265   36043 out.go:177] 🔎  Verifying Kubernetes components...
I1013 21:58:57.308321   36043 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1013 21:58:57.337940   36043 out.go:177]     ▪ Using image docker.io/kubernetesui/dashboard:v2.7.0
I1013 21:58:57.338088   36043 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1013 21:58:57.339311   36043 out.go:177]     ▪ Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I1013 21:58:57.339404   36043 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1013 21:58:57.339407   36043 addons.go:234] Setting addon default-storageclass=true in "minikube"
I1013 21:58:57.339413   36043 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
W1013 21:58:57.339414   36043 addons.go:243] addon default-storageclass should already be in state true
I1013 21:58:57.339437   36043 host.go:66] Checking if "minikube" exists ...
I1013 21:58:57.339493   36043 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1013 21:58:57.339752   36043 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1013 21:58:57.340099   36043 addons.go:431] installing /etc/kubernetes/addons/dashboard-ns.yaml
I1013 21:58:57.340106   36043 ssh_runner.go:362] scp dashboard/dashboard-ns.yaml --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I1013 21:58:57.340162   36043 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1013 21:58:57.370628   36043 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51373 SSHKeyPath:/home/atviks/.minikube/machines/minikube/id_rsa Username:docker}
I1013 21:58:57.371508   36043 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I1013 21:58:57.371522   36043 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1013 21:58:57.371597   36043 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1013 21:58:57.372072   36043 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51373 SSHKeyPath:/home/atviks/.minikube/machines/minikube/id_rsa Username:docker}
I1013 21:58:57.391164   36043 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:51373 SSHKeyPath:/home/atviks/.minikube/machines/minikube/id_rsa Username:docker}
I1013 21:58:57.403581   36043 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1013 21:58:57.414823   36043 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1013 21:58:57.438933   36043 api_server.go:52] waiting for apiserver process to appear ...
I1013 21:58:57.439007   36043 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1013 21:58:57.476476   36043 addons.go:431] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I1013 21:58:57.476491   36043 ssh_runner.go:362] scp dashboard/dashboard-clusterrole.yaml --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I1013 21:58:57.476538   36043 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1013 21:58:57.487213   36043 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1013 21:58:57.496598   36043 addons.go:431] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I1013 21:58:57.496627   36043 ssh_runner.go:362] scp dashboard/dashboard-clusterrolebinding.yaml --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I1013 21:58:57.660931   36043 addons.go:431] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I1013 21:58:57.660951   36043 ssh_runner.go:362] scp dashboard/dashboard-configmap.yaml --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I1013 21:58:57.679159   36043 addons.go:431] installing /etc/kubernetes/addons/dashboard-dp.yaml
I1013 21:58:57.679211   36043 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I1013 21:58:57.774400   36043 addons.go:431] installing /etc/kubernetes/addons/dashboard-role.yaml
I1013 21:58:57.774414   36043 ssh_runner.go:362] scp dashboard/dashboard-role.yaml --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I1013 21:58:57.791843   36043 addons.go:431] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I1013 21:58:57.791857   36043 ssh_runner.go:362] scp dashboard/dashboard-rolebinding.yaml --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I1013 21:58:57.806591   36043 addons.go:431] installing /etc/kubernetes/addons/dashboard-sa.yaml
I1013 21:58:57.806617   36043 ssh_runner.go:362] scp dashboard/dashboard-sa.yaml --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I1013 21:58:57.877911   36043 addons.go:431] installing /etc/kubernetes/addons/dashboard-secret.yaml
I1013 21:58:57.877925   36043 ssh_runner.go:362] scp dashboard/dashboard-secret.yaml --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I1013 21:58:57.893598   36043 addons.go:431] installing /etc/kubernetes/addons/dashboard-svc.yaml
I1013 21:58:57.893625   36043 ssh_runner.go:362] scp dashboard/dashboard-svc.yaml --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I1013 21:58:57.939139   36043 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1013 21:58:57.980155   36043 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
W1013 21:58:58.065091   36043 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1013 21:58:58.065200   36043 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1013 21:58:58.065256   36043 retry.go:31] will retry after 197.256254ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1013 21:58:58.065256   36043 retry.go:31] will retry after 355.833945ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1013 21:58:58.262858   36043 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W1013 21:58:58.284246   36043 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1013 21:58:58.284318   36043 retry.go:31] will retry after 356.176306ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1013 21:58:58.421871   36043 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1013 21:58:58.439963   36043 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1013 21:58:58.472731   36043 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1013 21:58:58.472761   36043 retry.go:31] will retry after 552.645558ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1013 21:58:58.641214   36043 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I1013 21:58:59.026207   36043 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1013 21:59:01.905667   36043 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (3.483768301s)
I1013 21:59:01.905712   36043 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (3.465730697s)
I1013 21:59:01.905732   36043 api_server.go:72] duration metric: took 4.600777561s to wait for apiserver process to appear ...
I1013 21:59:01.905735   36043 api_server.go:88] waiting for apiserver healthz status ...
I1013 21:59:01.905749   36043 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51372/healthz ...
I1013 21:59:01.908975   36043 api_server.go:279] https://127.0.0.1:51372/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1013 21:59:01.908984   36043 api_server.go:103] status: https://127.0.0.1:51372/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1013 21:59:02.406306   36043 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51372/healthz ...
I1013 21:59:02.467361   36043 api_server.go:279] https://127.0.0.1:51372/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1013 21:59:02.467407   36043 api_server.go:103] status: https://127.0.0.1:51372/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1013 21:59:02.606644   36043 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (3.965397804s)
I1013 21:59:02.606737   36043 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (3.580513051s)
I1013 21:59:02.610131   36043 out.go:177] 💡  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server

I1013 21:59:02.662932   36043 out.go:177] 🌟  Enabled addons: storage-provisioner, dashboard, default-storageclass
I1013 21:59:02.663735   36043 addons.go:510] duration metric: took 5.358810118s for enable addons: enabled=[storage-provisioner dashboard default-storageclass]
I1013 21:59:02.906660   36043 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:51372/healthz ...
I1013 21:59:02.966222   36043 api_server.go:279] https://127.0.0.1:51372/healthz returned 200:
ok
I1013 21:59:02.968059   36043 api_server.go:141] control plane version: v1.31.0
I1013 21:59:02.968084   36043 api_server.go:131] duration metric: took 1.062344062s to wait for apiserver health ...
I1013 21:59:02.968154   36043 system_pods.go:43] waiting for kube-system pods to appear ...
I1013 21:59:02.976073   36043 system_pods.go:59] 7 kube-system pods found
I1013 21:59:02.976096   36043 system_pods.go:61] "coredns-6f6b679f8f-8qrtn" [eab5b6d8-48c8-4b16-8e3e-8878f9e75ad5] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1013 21:59:02.976102   36043 system_pods.go:61] "etcd-minikube" [c14ecec7-592b-4450-ad38-b421c5b8206f] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1013 21:59:02.976107   36043 system_pods.go:61] "kube-apiserver-minikube" [212b7245-9812-40fc-b6ce-0caaf61b39dc] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1013 21:59:02.976111   36043 system_pods.go:61] "kube-controller-manager-minikube" [beaa083c-8185-4578-b234-f722a5f3022a] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1013 21:59:02.976114   36043 system_pods.go:61] "kube-proxy-ckhr5" [7506a22d-84f1-44e1-bb20-85f38cada47a] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1013 21:59:02.976120   36043 system_pods.go:61] "kube-scheduler-minikube" [06347274-a610-498f-aa59-29c3b58f8782] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1013 21:59:02.976123   36043 system_pods.go:61] "storage-provisioner" [59ad433d-7158-4c16-8778-38ca462bc0cf] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1013 21:59:02.976128   36043 system_pods.go:74] duration metric: took 7.969951ms to wait for pod list to return data ...
I1013 21:59:02.976177   36043 kubeadm.go:582] duration metric: took 5.671187447s to wait for: map[apiserver:true system_pods:true]
I1013 21:59:02.976212   36043 node_conditions.go:102] verifying NodePressure condition ...
I1013 21:59:02.983081   36043 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1013 21:59:02.983113   36043 node_conditions.go:123] node cpu capacity is 16
I1013 21:59:02.983124   36043 node_conditions.go:105] duration metric: took 6.908127ms to run NodePressure ...
I1013 21:59:02.983142   36043 start.go:241] waiting for startup goroutines ...
I1013 21:59:02.983148   36043 start.go:246] waiting for cluster config update ...
I1013 21:59:02.983157   36043 start.go:255] writing updated cluster config ...
I1013 21:59:02.983837   36043 ssh_runner.go:195] Run: rm -f paused
I1013 21:59:03.103805   36043 start.go:600] kubectl: 1.31.1, cluster: 1.31.0 (minor skew: 0)
I1013 21:59:03.105075   36043 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Oct 13 12:58:55 minikube dockerd[838]: time="2024-10-13T12:58:55.028427360Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Oct 13 12:58:55 minikube dockerd[838]: time="2024-10-13T12:58:55.644232420Z" level=info msg="Processing signal 'terminated'"
Oct 13 12:58:55 minikube dockerd[838]: time="2024-10-13T12:58:55.650209728Z" level=info msg="Loading containers: start."
Oct 13 12:58:55 minikube dockerd[838]: time="2024-10-13T12:58:55.755112526Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Oct 13 12:58:55 minikube dockerd[838]: time="2024-10-13T12:58:55.787186148Z" level=info msg="Loading containers: done."
Oct 13 12:58:55 minikube dockerd[838]: time="2024-10-13T12:58:55.798735047Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Oct 13 12:58:55 minikube dockerd[838]: time="2024-10-13T12:58:55.798778570Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Oct 13 12:58:55 minikube dockerd[838]: time="2024-10-13T12:58:55.798784676Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Oct 13 12:58:55 minikube dockerd[838]: time="2024-10-13T12:58:55.798788077Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Oct 13 12:58:55 minikube dockerd[838]: time="2024-10-13T12:58:55.798826874Z" level=info msg="Docker daemon" commit=3ab5c7d containerd-snapshotter=false storage-driver=overlay2 version=27.2.0
Oct 13 12:58:55 minikube dockerd[838]: time="2024-10-13T12:58:55.798881139Z" level=info msg="Daemon has completed initialization"
Oct 13 12:58:55 minikube dockerd[838]: time="2024-10-13T12:58:55.830049532Z" level=info msg="API listen on /var/run/docker.sock"
Oct 13 12:58:55 minikube dockerd[838]: time="2024-10-13T12:58:55.830077129Z" level=info msg="API listen on [::]:2376"
Oct 13 12:58:55 minikube dockerd[838]: time="2024-10-13T12:58:55.831219030Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Oct 13 12:58:55 minikube dockerd[838]: time="2024-10-13T12:58:55.831858251Z" level=info msg="Daemon shutdown complete"
Oct 13 12:58:55 minikube dockerd[838]: time="2024-10-13T12:58:55.832019602Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Oct 13 12:58:55 minikube systemd[1]: docker.service: Deactivated successfully.
Oct 13 12:58:55 minikube systemd[1]: Stopped Docker Application Container Engine.
Oct 13 12:58:55 minikube systemd[1]: Starting Docker Application Container Engine...
Oct 13 12:58:55 minikube dockerd[1124]: time="2024-10-13T12:58:55.865548978Z" level=info msg="Starting up"
Oct 13 12:58:55 minikube dockerd[1124]: time="2024-10-13T12:58:55.885208691Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Oct 13 12:58:55 minikube dockerd[1124]: time="2024-10-13T12:58:55.905704153Z" level=info msg="Loading containers: start."
Oct 13 12:58:56 minikube dockerd[1124]: time="2024-10-13T12:58:56.010362222Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Oct 13 12:58:56 minikube dockerd[1124]: time="2024-10-13T12:58:56.041061922Z" level=info msg="Loading containers: done."
Oct 13 12:58:56 minikube dockerd[1124]: time="2024-10-13T12:58:56.052987337Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Oct 13 12:58:56 minikube dockerd[1124]: time="2024-10-13T12:58:56.053022153Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Oct 13 12:58:56 minikube dockerd[1124]: time="2024-10-13T12:58:56.053027481Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Oct 13 12:58:56 minikube dockerd[1124]: time="2024-10-13T12:58:56.053030934Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Oct 13 12:58:56 minikube dockerd[1124]: time="2024-10-13T12:58:56.053045468Z" level=info msg="Docker daemon" commit=3ab5c7d containerd-snapshotter=false storage-driver=overlay2 version=27.2.0
Oct 13 12:58:56 minikube dockerd[1124]: time="2024-10-13T12:58:56.053082476Z" level=info msg="Daemon has completed initialization"
Oct 13 12:58:56 minikube dockerd[1124]: time="2024-10-13T12:58:56.080752240Z" level=info msg="API listen on /var/run/docker.sock"
Oct 13 12:58:56 minikube dockerd[1124]: time="2024-10-13T12:58:56.080775961Z" level=info msg="API listen on [::]:2376"
Oct 13 12:58:56 minikube systemd[1]: Started Docker Application Container Engine.
Oct 13 12:58:56 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Oct 13 12:58:56 minikube cri-dockerd[1419]: time="2024-10-13T12:58:56Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Oct 13 12:58:56 minikube cri-dockerd[1419]: time="2024-10-13T12:58:56Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Oct 13 12:58:56 minikube cri-dockerd[1419]: time="2024-10-13T12:58:56Z" level=info msg="Start docker client with request timeout 0s"
Oct 13 12:58:56 minikube cri-dockerd[1419]: time="2024-10-13T12:58:56Z" level=info msg="Hairpin mode is set to hairpin-veth"
Oct 13 12:58:56 minikube cri-dockerd[1419]: time="2024-10-13T12:58:56Z" level=info msg="Loaded network plugin cni"
Oct 13 12:58:56 minikube cri-dockerd[1419]: time="2024-10-13T12:58:56Z" level=info msg="Docker cri networking managed by network plugin cni"
Oct 13 12:58:56 minikube cri-dockerd[1419]: time="2024-10-13T12:58:56Z" level=info msg="Setting cgroupDriver cgroupfs"
Oct 13 12:58:56 minikube cri-dockerd[1419]: time="2024-10-13T12:58:56Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Oct 13 12:58:56 minikube cri-dockerd[1419]: time="2024-10-13T12:58:56Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Oct 13 12:58:56 minikube cri-dockerd[1419]: time="2024-10-13T12:58:56Z" level=info msg="Start cri-dockerd grpc backend"
Oct 13 12:58:56 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Oct 13 12:58:57 minikube cri-dockerd[1419]: time="2024-10-13T12:58:57Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-695b96c756-mq464_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"1a8deb4d2bb2ca63156676f3334f11db2e7e75cbface05dfb42da952fb85b6b3\""
Oct 13 12:58:57 minikube cri-dockerd[1419]: time="2024-10-13T12:58:57Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-6f6b679f8f-8qrtn_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"8426bd0109baaab14a672d654c0418c51ab0d1aa030aad9e12f0d4aa327ea836\""
Oct 13 12:58:57 minikube cri-dockerd[1419]: time="2024-10-13T12:58:57Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-c5db448b4-drhwx_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"65e5df4e96c4088ce368ed3880273d8b293867f52ba0a9879c851bdb7859ee88\""
Oct 13 12:58:57 minikube cri-dockerd[1419]: time="2024-10-13T12:58:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0fbb969659b63b8118d163df6628e8382c89fab225cd433269ca08c2404a895e/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 13 12:58:57 minikube cri-dockerd[1419]: time="2024-10-13T12:58:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/399dd1a0765e3e6aeb843475a22242ff917573352bcce7bf8523e8fdc4bd7304/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 13 12:58:57 minikube cri-dockerd[1419]: time="2024-10-13T12:58:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6c3303c0ee977970331c0a7ab920000d7f1cd1aa3bfdc9bc2ba485d0fae43723/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 13 12:58:57 minikube cri-dockerd[1419]: time="2024-10-13T12:58:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7c45d7e2a5616d7cdccd70bcc835721e6e71dfc8454292bee3ae7b62cc3f03fa/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 13 12:59:01 minikube cri-dockerd[1419]: time="2024-10-13T12:59:01Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Oct 13 12:59:02 minikube cri-dockerd[1419]: time="2024-10-13T12:59:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/48b868f07e3e44141c28d80947172ebe5579b017cb3fa46fc10f8513eba3f253/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 13 12:59:02 minikube cri-dockerd[1419]: time="2024-10-13T12:59:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/44be68b2e2cfb4120c41eb4e527c499ca495c34053404801bc486934b3239215/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 13 12:59:03 minikube cri-dockerd[1419]: time="2024-10-13T12:59:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1782f54cd3445b6f76ba73a3d8286ba0aa5700e7990418d9c9b8412213e803a2/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 13 12:59:03 minikube cri-dockerd[1419]: time="2024-10-13T12:59:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/79dfa0649b735ce9a4e36d4870a4fc32e02d81818627918ed8bb0062b1da38cf/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Oct 13 12:59:03 minikube cri-dockerd[1419]: time="2024-10-13T12:59:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/19af0e94869542fb194206775ac37cc27d20fad8f082d2cb71509796f818dff7/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 13 12:59:24 minikube dockerd[1124]: time="2024-10-13T12:59:24.549656370Z" level=info msg="ignoring event" container=675e7866f0690ba25c58273ba0ec3ba34fe8e9e80c45c05dfc5a3d29fa7641da module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 13 12:59:25 minikube dockerd[1124]: time="2024-10-13T12:59:25.057210494Z" level=info msg="ignoring event" container=ecc4a7ba70342c4f5428398d951b71d958d6121b8a039edc6d275145ad5cdebd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE                                                                                                  CREATED              STATE               NAME                        ATTEMPT             POD ID              POD
7d2430ff59267       07655ddf2eebe                                                                                          21 seconds ago       Running             kubernetes-dashboard        2                   1782f54cd3445       kubernetes-dashboard-695b96c756-mq464
1f01d631846dc       6e38f40d628db                                                                                          28 seconds ago       Running             storage-provisioner         3                   44be68b2e2cfb       storage-provisioner
62c60cd9629a7       115053965e86b                                                                                          About a minute ago   Running             dashboard-metrics-scraper   1                   19af0e9486954       dashboard-metrics-scraper-c5db448b4-drhwx
b292504a585a9       cbb01a7bd410d                                                                                          About a minute ago   Running             coredns                     1                   79dfa0649b735       coredns-6f6b679f8f-8qrtn
ecc4a7ba70342       07655ddf2eebe                                                                                          About a minute ago   Exited              kubernetes-dashboard        1                   1782f54cd3445       kubernetes-dashboard-695b96c756-mq464
95c999d899d12       ad83b2ca7b09e                                                                                          About a minute ago   Running             kube-proxy                  1                   48b868f07e3e4       kube-proxy-ckhr5
675e7866f0690       6e38f40d628db                                                                                          About a minute ago   Exited              storage-provisioner         2                   44be68b2e2cfb       storage-provisioner
607963d799d26       2e96e5913fc06                                                                                          About a minute ago   Running             etcd                        1                   0fbb969659b63       etcd-minikube
1fe4186aab2f6       1766f54c897f0                                                                                          About a minute ago   Running             kube-scheduler              1                   7c45d7e2a5616       kube-scheduler-minikube
4dd51d4673c4f       045733566833c                                                                                          About a minute ago   Running             kube-controller-manager     1                   6c3303c0ee977       kube-controller-manager-minikube
179e9dad5b65a       604f5db92eaa8                                                                                          About a minute ago   Running             kube-apiserver              1                   399dd1a0765e3       kube-apiserver-minikube
cb848c83c7b4d       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c   21 minutes ago       Exited              dashboard-metrics-scraper   0                   65e5df4e96c40       dashboard-metrics-scraper-c5db448b4-drhwx
aad18cf8f8c8a       cbb01a7bd410d                                                                                          25 minutes ago       Exited              coredns                     0                   8426bd0109baa       coredns-6f6b679f8f-8qrtn
5a02c15bb8f1e       ad83b2ca7b09e                                                                                          25 minutes ago       Exited              kube-proxy                  0                   37435e4e1d93b       kube-proxy-ckhr5
0b1648db4a667       604f5db92eaa8                                                                                          25 minutes ago       Exited              kube-apiserver              0                   dbf8ce369c453       kube-apiserver-minikube
f16361076e65d       1766f54c897f0                                                                                          25 minutes ago       Exited              kube-scheduler              0                   f52f06d215b93       kube-scheduler-minikube
e99f9344f20a8       2e96e5913fc06                                                                                          25 minutes ago       Exited              etcd                        0                   41b1162e0c2f1       etcd-minikube
dfe228f826626       045733566833c                                                                                          25 minutes ago       Exited              kube-controller-manager     0                   d26891a012fc6       kube-controller-manager-minikube


==> coredns [aad18cf8f8c8] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = bc3517d4f3278b81bf2c83d2a108d2cc34dd9740a8d4d2f148e6cdf32f99db2622d2c6c604e6b1b87943d2bfc71a685437bc1f5e7b016d830d09bd3220d1c1fc
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:32858 - 36959 "HINFO IN 6824736090612034549.298217729101205938. udp 56 false 512" NXDOMAIN qr,rd,ra 56 0.088050395s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1803010308]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (13-Oct-2024 12:34:45.340) (total time: 21046ms):
Trace[1803010308]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21045ms (12:35:06.384)
Trace[1803010308]: [21.046085181s] [21.046085181s] END
[INFO] plugin/kubernetes: Trace[56651622]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (13-Oct-2024 12:34:45.340) (total time: 21046ms):
Trace[56651622]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21045ms (12:35:06.384)
Trace[56651622]: [21.046150479s] [21.046150479s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1264718382]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (13-Oct-2024 12:34:45.340) (total time: 21046ms):
Trace[1264718382]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21045ms (12:35:06.384)
Trace[1264718382]: [21.04625981s] [21.04625981s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [b292504a585a] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = bc3517d4f3278b81bf2c83d2a108d2cc34dd9740a8d4d2f148e6cdf32f99db2622d2c6c604e6b1b87943d2bfc71a685437bc1f5e7b016d830d09bd3220d1c1fc
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:51227 - 65115 "HINFO IN 3227643123809307433.563387074339691252. udp 56 false 512" NXDOMAIN qr,rd,ra 56 0.035122656s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1889066527]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (13-Oct-2024 12:59:03.997) (total time: 21054ms):
Trace[1889066527]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21054ms (12:59:25.051)
Trace[1889066527]: [21.05440784s] [21.05440784s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1003513445]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (13-Oct-2024 12:59:03.997) (total time: 21054ms):
Trace[1003513445]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21054ms (12:59:25.051)
Trace[1003513445]: [21.054596083s] [21.054596083s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[731588680]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (13-Oct-2024 12:59:03.997) (total time: 21054ms):
Trace[731588680]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21054ms (12:59:25.051)
Trace[731588680]: [21.054734153s] [21.054734153s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_10_13T21_34_39_0700
                    minikube.k8s.io/version=v1.34.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 13 Oct 2024 12:34:36 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sun, 13 Oct 2024 13:00:02 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sun, 13 Oct 2024 12:59:01 +0000   Sun, 13 Oct 2024 12:34:34 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sun, 13 Oct 2024 12:59:01 +0000   Sun, 13 Oct 2024 12:34:34 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sun, 13 Oct 2024 12:59:01 +0000   Sun, 13 Oct 2024 12:34:34 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sun, 13 Oct 2024 12:59:01 +0000   Sun, 13 Oct 2024 12:34:37 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.220.2
  Hostname:    minikube
Capacity:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8012128Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8012128Ki
  pods:               110
System Info:
  Machine ID:                 658fb8e7b5c843eaa785ca09920fd7b9
  System UUID:                658fb8e7b5c843eaa785ca09920fd7b9
  Boot ID:                    6122eea1-bca4-45c5-bdbb-8106a7cd22e3
  Kernel Version:             5.15.153.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-6f6b679f8f-8qrtn                     100m (0%)     0 (0%)      70Mi (0%)        170Mi (2%)     25m
  kube-system                 etcd-minikube                                100m (0%)     0 (0%)      100Mi (1%)       0 (0%)         25m
  kube-system                 kube-apiserver-minikube                      250m (1%)     0 (0%)      0 (0%)           0 (0%)         25m
  kube-system                 kube-controller-manager-minikube             200m (1%)     0 (0%)      0 (0%)           0 (0%)         25m
  kube-system                 kube-proxy-ckhr5                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         25m
  kube-system                 kube-scheduler-minikube                      100m (0%)     0 (0%)      0 (0%)           0 (0%)         25m
  kube-system                 storage-provisioner                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         25m
  kubernetes-dashboard        dashboard-metrics-scraper-c5db448b4-drhwx    0 (0%)        0 (0%)      0 (0%)           0 (0%)         21m
  kubernetes-dashboard        kubernetes-dashboard-695b96c756-mq464        0 (0%)        0 (0%)      0 (0%)           0 (0%)         21m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (4%)   0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           25m                kube-proxy       
  Normal   Starting                           60s                kube-proxy       
  Normal   NodeHasSufficientPID               25m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Warning  PossibleMemoryBackedVolumesOnDisk  25m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Warning  CgroupV1                           25m                kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Normal   NodeAllocatableEnforced            25m                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            25m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              25m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   Starting                           25m                kubelet          Starting kubelet.
  Normal   RegisteredNode                     25m                node-controller  Node minikube event: Registered Node minikube in Controller
  Warning  PossibleMemoryBackedVolumesOnDisk  68s                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           68s                kubelet          Starting kubelet.
  Warning  CgroupV1                           68s                kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Normal   NodeHasSufficientMemory            67s (x7 over 67s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              67s (x7 over 67s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               67s (x7 over 67s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            67s                kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode                     60s                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +1.468657] FS-Cache: Duplicate cookie detected
[  +0.000350] FS-Cache: O-cookie c=00000005 [p=00000002 fl=222 nc=0 na=1]
[  +0.000322] FS-Cache: O-cookie d=000000004a424152{9P.session} n=000000006819e486
[  +0.000353] FS-Cache: O-key=[10] '34323934393337343636'
[  +0.000242] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.000289] FS-Cache: N-cookie d=000000004a424152{9P.session} n=00000000279c0335
[  +0.000410] FS-Cache: N-key=[10] '34323934393337343636'
[  +0.580285] /sbin/ldconfig: 
[  +0.000014] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.175478] /sbin/ldconfig.real: 
[  +0.000003] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.200750] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.008129] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000512] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000406] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000504] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001442] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000524] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000436] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000664] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +2.351335] misc dxg: dxgk: dxgkio_reserve_gpu_va: Ioctl failed: -75
[Oct13 12:29] /sbin/ldconfig: 
[  +0.000005] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.112135] WSL (1) ERROR: ConfigApplyWindowsLibPath:2531: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000005]  failed 2
[  +0.017565] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Tokyo not found. Is the tzdata package installed?
[  +0.118031] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.008002] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000524] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000388] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000624] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001459] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000401] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000478] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000573] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.193777] misc dxg: dxgk: dxgkio_reserve_gpu_va: Ioctl failed: -75
[  +5.452878] /sbin/ldconfig: 
[  +0.000004] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.032767] WSL (1) ERROR: ConfigApplyWindowsLibPath:2531: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000003]  failed 2
[  +0.010628] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Tokyo not found. Is the tzdata package installed?
[  +0.069864] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.004077] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000472] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000367] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000507] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000966] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000363] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000394] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000615] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.047947] misc dxg: dxgk: dxgkio_reserve_gpu_va: Ioctl failed: -75
[  +1.863356] new mount options do not match the existing superblock, will be ignored
[  +0.000266] netlink: 'init': attribute type 4 has an invalid length.
[Oct13 12:34] tmpfs: Unknown parameter 'noswap'
[  +5.877386] tmpfs: Unknown parameter 'noswap'
[Oct13 12:58] tmpfs: Unknown parameter 'noswap'


==> etcd [607963d799d2] <==
{"level":"warn","ts":"2024-10-13T12:58:58.370281Z","caller":"embed/config.go:687","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-10-13T12:58:58.370356Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.220.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.220.2:2380","--initial-cluster=minikube=https://192.168.220.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.220.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.220.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2024-10-13T12:58:58.370404Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2024-10-13T12:58:58.370418Z","caller":"embed/config.go:687","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-10-13T12:58:58.370422Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.220.2:2380"]}
{"level":"info","ts":"2024-10-13T12:58:58.370443Z","caller":"embed/etcd.go:496","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-10-13T12:58:58.371377Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.220.2:2379"]}
{"level":"info","ts":"2024-10-13T12:58:58.371546Z","caller":"embed/etcd.go:310","msg":"starting an etcd server","etcd-version":"3.5.15","git-sha":"9a5533382","go-version":"go1.21.12","go-os":"linux","go-arch":"amd64","max-cpu-set":16,"max-cpu-available":16,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.220.2:2380"],"listen-peer-urls":["https://192.168.220.2:2380"],"advertise-client-urls":["https://192.168.220.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.220.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-10-13T12:58:58.375299Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"3.615324ms"}
{"level":"info","ts":"2024-10-13T12:58:58.382241Z","caller":"etcdserver/server.go:532","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2024-10-13T12:58:58.460088Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"818483a0e0a44de1","local-member-id":"b52bad7105151d3d","commit-index":1900}
{"level":"info","ts":"2024-10-13T12:58:58.460342Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b52bad7105151d3d switched to configuration voters=()"}
{"level":"info","ts":"2024-10-13T12:58:58.460445Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b52bad7105151d3d became follower at term 2"}
{"level":"info","ts":"2024-10-13T12:58:58.460482Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft b52bad7105151d3d [peers: [], term: 2, commit: 1900, applied: 0, lastindex: 1900, lastterm: 2]"}
{"level":"warn","ts":"2024-10-13T12:58:58.462159Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-10-13T12:58:58.462743Z","caller":"mvcc/kvstore.go:341","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":1168}
{"level":"info","ts":"2024-10-13T12:58:58.465706Z","caller":"mvcc/kvstore.go:418","msg":"kvstore restored","current-rev":1600}
{"level":"info","ts":"2024-10-13T12:58:58.466660Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-10-13T12:58:58.468255Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"b52bad7105151d3d","timeout":"7s"}
{"level":"info","ts":"2024-10-13T12:58:58.468568Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"b52bad7105151d3d"}
{"level":"info","ts":"2024-10-13T12:58:58.468619Z","caller":"etcdserver/server.go:867","msg":"starting etcd server","local-member-id":"b52bad7105151d3d","local-server-version":"3.5.15","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-10-13T12:58:58.468682Z","caller":"etcdserver/server.go:767","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2024-10-13T12:58:58.468785Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-10-13T12:58:58.468843Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-10-13T12:58:58.468852Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-10-13T12:58:58.468939Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b52bad7105151d3d switched to configuration voters=(13054718645791694141)"}
{"level":"info","ts":"2024-10-13T12:58:58.469034Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"818483a0e0a44de1","local-member-id":"b52bad7105151d3d","added-peer-id":"b52bad7105151d3d","added-peer-peer-urls":["https://192.168.220.2:2380"]}
{"level":"info","ts":"2024-10-13T12:58:58.469155Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"818483a0e0a44de1","local-member-id":"b52bad7105151d3d","cluster-version":"3.5"}
{"level":"info","ts":"2024-10-13T12:58:58.469135Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-10-13T12:58:58.469184Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-10-13T12:58:58.471526Z","caller":"embed/etcd.go:728","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-10-13T12:58:58.471677Z","caller":"embed/etcd.go:599","msg":"serving peer traffic","address":"192.168.220.2:2380"}
{"level":"info","ts":"2024-10-13T12:58:58.471698Z","caller":"embed/etcd.go:571","msg":"cmux::serve","address":"192.168.220.2:2380"}
{"level":"info","ts":"2024-10-13T12:58:58.471805Z","caller":"embed/etcd.go:279","msg":"now serving peer/client/metrics","local-member-id":"b52bad7105151d3d","initial-advertise-peer-urls":["https://192.168.220.2:2380"],"listen-peer-urls":["https://192.168.220.2:2380"],"advertise-client-urls":["https://192.168.220.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.220.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-10-13T12:58:58.471845Z","caller":"embed/etcd.go:870","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-10-13T12:59:00.261567Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b52bad7105151d3d is starting a new election at term 2"}
{"level":"info","ts":"2024-10-13T12:59:00.261655Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b52bad7105151d3d became pre-candidate at term 2"}
{"level":"info","ts":"2024-10-13T12:59:00.261689Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b52bad7105151d3d received MsgPreVoteResp from b52bad7105151d3d at term 2"}
{"level":"info","ts":"2024-10-13T12:59:00.261700Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b52bad7105151d3d became candidate at term 3"}
{"level":"info","ts":"2024-10-13T12:59:00.261704Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b52bad7105151d3d received MsgVoteResp from b52bad7105151d3d at term 3"}
{"level":"info","ts":"2024-10-13T12:59:00.261713Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b52bad7105151d3d became leader at term 3"}
{"level":"info","ts":"2024-10-13T12:59:00.261720Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: b52bad7105151d3d elected leader b52bad7105151d3d at term 3"}
{"level":"info","ts":"2024-10-13T12:59:00.263520Z","caller":"etcdserver/server.go:2118","msg":"published local member to cluster through raft","local-member-id":"b52bad7105151d3d","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.220.2:2379]}","request-path":"/0/members/b52bad7105151d3d/attributes","cluster-id":"818483a0e0a44de1","publish-timeout":"7s"}
{"level":"info","ts":"2024-10-13T12:59:00.263532Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-10-13T12:59:00.263580Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-10-13T12:59:00.263794Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-10-13T12:59:00.263820Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-10-13T12:59:00.264239Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-10-13T12:59:00.264255Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-10-13T12:59:00.264742Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.220.2:2379"}
{"level":"info","ts":"2024-10-13T12:59:00.264860Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}


==> etcd [e99f9344f20a] <==
{"level":"info","ts":"2024-10-13T12:34:34.529745Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b52bad7105151d3d became follower at term 1"}
{"level":"info","ts":"2024-10-13T12:34:34.529945Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b52bad7105151d3d switched to configuration voters=(13054718645791694141)"}
{"level":"warn","ts":"2024-10-13T12:34:34.534422Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-10-13T12:34:34.535460Z","caller":"mvcc/kvstore.go:418","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2024-10-13T12:34:34.536163Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-10-13T12:34:34.537543Z","caller":"etcdserver/server.go:867","msg":"starting etcd server","local-member-id":"b52bad7105151d3d","local-server-version":"3.5.15","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-10-13T12:34:34.537687Z","caller":"etcdserver/server.go:751","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"b52bad7105151d3d","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2024-10-13T12:34:34.538201Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-10-13T12:34:34.538311Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-10-13T12:34:34.538343Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-10-13T12:34:34.538701Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b52bad7105151d3d switched to configuration voters=(13054718645791694141)"}
{"level":"info","ts":"2024-10-13T12:34:34.538725Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-10-13T12:34:34.538866Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"818483a0e0a44de1","local-member-id":"b52bad7105151d3d","added-peer-id":"b52bad7105151d3d","added-peer-peer-urls":["https://192.168.220.2:2380"]}
{"level":"info","ts":"2024-10-13T12:34:34.609658Z","caller":"embed/etcd.go:728","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-10-13T12:34:34.609835Z","caller":"embed/etcd.go:599","msg":"serving peer traffic","address":"192.168.220.2:2380"}
{"level":"info","ts":"2024-10-13T12:34:34.609873Z","caller":"embed/etcd.go:571","msg":"cmux::serve","address":"192.168.220.2:2380"}
{"level":"info","ts":"2024-10-13T12:34:34.610084Z","caller":"embed/etcd.go:279","msg":"now serving peer/client/metrics","local-member-id":"b52bad7105151d3d","initial-advertise-peer-urls":["https://192.168.220.2:2380"],"listen-peer-urls":["https://192.168.220.2:2380"],"advertise-client-urls":["https://192.168.220.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.220.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-10-13T12:34:34.610123Z","caller":"embed/etcd.go:870","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-10-13T12:34:35.530915Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b52bad7105151d3d is starting a new election at term 1"}
{"level":"info","ts":"2024-10-13T12:34:35.530970Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b52bad7105151d3d became pre-candidate at term 1"}
{"level":"info","ts":"2024-10-13T12:34:35.530997Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b52bad7105151d3d received MsgPreVoteResp from b52bad7105151d3d at term 1"}
{"level":"info","ts":"2024-10-13T12:34:35.531006Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b52bad7105151d3d became candidate at term 2"}
{"level":"info","ts":"2024-10-13T12:34:35.531010Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b52bad7105151d3d received MsgVoteResp from b52bad7105151d3d at term 2"}
{"level":"info","ts":"2024-10-13T12:34:35.531016Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b52bad7105151d3d became leader at term 2"}
{"level":"info","ts":"2024-10-13T12:34:35.531022Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: b52bad7105151d3d elected leader b52bad7105151d3d at term 2"}
{"level":"info","ts":"2024-10-13T12:34:35.531703Z","caller":"etcdserver/server.go:2118","msg":"published local member to cluster through raft","local-member-id":"b52bad7105151d3d","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.220.2:2379]}","request-path":"/0/members/b52bad7105151d3d/attributes","cluster-id":"818483a0e0a44de1","publish-timeout":"7s"}
{"level":"info","ts":"2024-10-13T12:34:35.531722Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-10-13T12:34:35.531763Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-10-13T12:34:35.531763Z","caller":"etcdserver/server.go:2629","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2024-10-13T12:34:35.532052Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-10-13T12:34:35.532095Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-10-13T12:34:35.532219Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"818483a0e0a44de1","local-member-id":"b52bad7105151d3d","cluster-version":"3.5"}
{"level":"info","ts":"2024-10-13T12:34:35.532272Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-10-13T12:34:35.532319Z","caller":"etcdserver/server.go:2653","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2024-10-13T12:34:35.532558Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-10-13T12:34:35.532596Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-10-13T12:34:35.533079Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.220.2:2379"}
{"level":"info","ts":"2024-10-13T12:34:35.533088Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"warn","ts":"2024-10-13T12:39:04.430519Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"239.440737ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2024-10-13T12:39:04.430572Z","caller":"traceutil/trace.go:171","msg":"trace[521247806] transaction","detail":"{read_only:false; response_revision:652; number_of_response:1; }","duration":"166.458777ms","start":"2024-10-13T12:39:04.264101Z","end":"2024-10-13T12:39:04.430560Z","steps":["trace[521247806] 'process raft request'  (duration: 133.408834ms)","trace[521247806] 'compare'  (duration: 32.84031ms)"],"step_count":2}
{"level":"info","ts":"2024-10-13T12:39:04.430613Z","caller":"traceutil/trace.go:171","msg":"trace[485250031] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:651; }","duration":"239.579077ms","start":"2024-10-13T12:39:04.191026Z","end":"2024-10-13T12:39:04.430605Z","steps":["trace[485250031] 'range keys from in-memory index tree'  (duration: 239.4308ms)"],"step_count":1}
{"level":"info","ts":"2024-10-13T12:44:35.525529Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":688}
{"level":"info","ts":"2024-10-13T12:44:35.529183Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":688,"took":"3.36252ms","hash":287047666,"current-db-size-bytes":1896448,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1896448,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2024-10-13T12:44:35.529220Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":287047666,"revision":688,"compact-revision":-1}
{"level":"info","ts":"2024-10-13T12:49:35.524111Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":928}
{"level":"info","ts":"2024-10-13T12:49:35.526218Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":928,"took":"1.774309ms","hash":625856025,"current-db-size-bytes":1896448,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1052672,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2024-10-13T12:49:35.526306Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":625856025,"revision":928,"compact-revision":688}
{"level":"info","ts":"2024-10-13T12:54:35.518236Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1168}
{"level":"info","ts":"2024-10-13T12:54:35.520996Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":1168,"took":"1.89611ms","hash":2130362057,"current-db-size-bytes":1896448,"current-db-size":"1.9 MB","current-db-size-in-use-bytes":1040384,"current-db-size-in-use":"1.0 MB"}
{"level":"info","ts":"2024-10-13T12:54:35.521035Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2130362057,"revision":1168,"compact-revision":928}
{"level":"info","ts":"2024-10-13T12:58:36.859865Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-10-13T12:58:36.860038Z","caller":"embed/etcd.go:377","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.220.2:2380"],"advertise-client-urls":["https://192.168.220.2:2379"]}
{"level":"warn","ts":"2024-10-13T12:58:36.860274Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-10-13T12:58:36.860446Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-10-13T12:58:36.960054Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.220.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-10-13T12:58:36.960135Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.220.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-10-13T12:58:36.960267Z","caller":"etcdserver/server.go:1521","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"b52bad7105151d3d","current-leader-member-id":"b52bad7105151d3d"}
{"level":"info","ts":"2024-10-13T12:58:36.965891Z","caller":"embed/etcd.go:581","msg":"stopping serving peer traffic","address":"192.168.220.2:2380"}
{"level":"info","ts":"2024-10-13T12:58:36.966536Z","caller":"embed/etcd.go:586","msg":"stopped serving peer traffic","address":"192.168.220.2:2380"}
{"level":"info","ts":"2024-10-13T12:58:36.966628Z","caller":"embed/etcd.go:379","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.220.2:2380"],"advertise-client-urls":["https://192.168.220.2:2379"]}


==> kernel <==
 13:00:04 up  1:08,  0 users,  load average: 0.83, 0.37, 0.23
Linux minikube 5.15.153.1-microsoft-standard-WSL2 #1 SMP Fri Mar 29 23:14:13 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [0b1648db4a66] <==
W1013 12:58:42.491714       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:42.520128       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:42.546868       1 logging.go:55] [core] [Channel #2 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:42.583229       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:42.646434       1 logging.go:55] [core] [Channel #5 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:42.656026       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:42.667609       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:42.671873       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:42.759235       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:44.947917       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:45.111074       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:45.163807       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:45.172156       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:45.262274       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:45.338094       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:45.422876       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:45.442806       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:45.460254       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:45.463666       1 logging.go:55] [core] [Channel #181 SubChannel #182]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:45.491799       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:45.541060       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:45.563001       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:45.587625       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:45.603334       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:45.642946       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:45.772067       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:45.785713       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:45.815766       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:45.822403       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:45.854506       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:45.896889       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.003299       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.013972       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.023802       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.044838       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.049194       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.050414       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.067836       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.139286       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.174214       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.177619       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.184239       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.199251       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.240677       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.283011       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.285335       1 logging.go:55] [core] [Channel #1 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.351657       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.361388       1 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.374011       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.395524       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.444993       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.470375       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.661125       1 logging.go:55] [core] [Channel #2 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.663760       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.684490       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.790294       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.825386       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.853897       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.870645       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1013 12:58:46.873967       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [179e9dad5b65] <==
W1013 12:59:00.714326       1 genericapiserver.go:765] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
I1013 12:59:00.714565       1 handler.go:286] Adding GroupVersion events.k8s.io v1 to ResourceManager
W1013 12:59:00.714585       1 genericapiserver.go:765] Skipping API events.k8s.io/v1beta1 because it has no resources.
I1013 12:59:00.720723       1 handler.go:286] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W1013 12:59:00.720754       1 genericapiserver.go:765] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I1013 12:59:01.092966       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1013 12:59:01.092979       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1013 12:59:01.093149       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I1013 12:59:01.093182       1 secure_serving.go:213] Serving securely on [::]:8443
I1013 12:59:01.093223       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1013 12:59:01.093261       1 controller.go:78] Starting OpenAPI AggregationController
I1013 12:59:01.093340       1 local_available_controller.go:156] Starting LocalAvailability controller
I1013 12:59:01.093364       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I1013 12:59:01.093368       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1013 12:59:01.093371       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I1013 12:59:01.093375       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I1013 12:59:01.093369       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I1013 12:59:01.093349       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I1013 12:59:01.093396       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1013 12:59:01.093398       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1013 12:59:01.093402       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1013 12:59:01.093412       1 system_namespaces_controller.go:66] Starting system namespaces controller
I1013 12:59:01.093489       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1013 12:59:01.093510       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I1013 12:59:01.093417       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1013 12:59:01.093380       1 controller.go:119] Starting legacy_token_tracking_controller
I1013 12:59:01.093553       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I1013 12:59:01.093362       1 aggregator.go:169] waiting for initial CRD sync...
I1013 12:59:01.093646       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I1013 12:59:01.093678       1 naming_controller.go:294] Starting NamingConditionController
I1013 12:59:01.093679       1 controller.go:90] Starting OpenAPI V3 controller
I1013 12:59:01.093697       1 establishing_controller.go:81] Starting EstablishingController
I1013 12:59:01.093714       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1013 12:59:01.093730       1 crd_finalizer.go:269] Starting CRDFinalizer
I1013 12:59:01.093733       1 controller.go:142] Starting OpenAPI controller
I1013 12:59:01.093732       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1013 12:59:01.093763       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1013 12:59:01.093893       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I1013 12:59:01.093900       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1013 12:59:01.167887       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I1013 12:59:01.167920       1 policy_source.go:224] refreshing policies
I1013 12:59:01.167953       1 shared_informer.go:320] Caches are synced for node_authorizer
I1013 12:59:01.259925       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1013 12:59:01.259985       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I1013 12:59:01.260205       1 shared_informer.go:320] Caches are synced for crd-autoregister
I1013 12:59:01.260226       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1013 12:59:01.260243       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1013 12:59:01.260260       1 aggregator.go:171] initial CRD sync complete...
I1013 12:59:01.260272       1 autoregister_controller.go:144] Starting autoregister controller
I1013 12:59:01.260280       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1013 12:59:01.260287       1 cache.go:39] Caches are synced for autoregister controller
I1013 12:59:01.260474       1 cache.go:39] Caches are synced for LocalAvailability controller
I1013 12:59:01.260759       1 shared_informer.go:320] Caches are synced for configmaps
I1013 12:59:01.260780       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1013 12:59:01.265139       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I1013 12:59:01.269550       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
E1013 12:59:01.270945       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I1013 12:59:01.801181       1 controller.go:615] quota admission added evaluator for: endpoints
I1013 12:59:02.096800       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1013 12:59:04.770490       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io


==> kube-controller-manager [4dd51d4673c4] <==
I1013 12:59:04.276786       1 shared_informer.go:320] Caches are synced for PVC protection
I1013 12:59:04.277332       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1013 12:59:04.277430       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I1013 12:59:04.282375       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I1013 12:59:04.287660       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I1013 12:59:04.287839       1 shared_informer.go:320] Caches are synced for expand
I1013 12:59:04.296494       1 shared_informer.go:320] Caches are synced for TTL
I1013 12:59:04.317914       1 shared_informer.go:320] Caches are synced for service account
I1013 12:59:04.360246       1 shared_informer.go:320] Caches are synced for node
I1013 12:59:04.360317       1 range_allocator.go:171] "Sending events to api server" logger="node-ipam-controller"
I1013 12:59:04.360336       1 range_allocator.go:177] "Starting range CIDR allocator" logger="node-ipam-controller"
I1013 12:59:04.360339       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I1013 12:59:04.360342       1 shared_informer.go:320] Caches are synced for cidrallocator
I1013 12:59:04.360402       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1013 12:59:04.362824       1 shared_informer.go:320] Caches are synced for persistent volume
I1013 12:59:04.362857       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I1013 12:59:04.364036       1 shared_informer.go:320] Caches are synced for PV protection
I1013 12:59:04.365231       1 shared_informer.go:320] Caches are synced for attach detach
I1013 12:59:04.365261       1 shared_informer.go:320] Caches are synced for daemon sets
I1013 12:59:04.365436       1 shared_informer.go:320] Caches are synced for namespace
I1013 12:59:04.365497       1 shared_informer.go:320] Caches are synced for taint
I1013 12:59:04.365601       1 node_lifecycle_controller.go:1232] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1013 12:59:04.365665       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1013 12:59:04.365708       1 node_lifecycle_controller.go:1078] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1013 12:59:04.366779       1 shared_informer.go:320] Caches are synced for TTL after finished
I1013 12:59:04.368143       1 shared_informer.go:320] Caches are synced for ReplicationController
I1013 12:59:04.369401       1 shared_informer.go:320] Caches are synced for ephemeral
I1013 12:59:04.369436       1 shared_informer.go:320] Caches are synced for ReplicaSet
I1013 12:59:04.369515       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="40.758µs"
I1013 12:59:04.369566       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I1013 12:59:04.369594       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="34.036µs"
I1013 12:59:04.370144       1 shared_informer.go:320] Caches are synced for stateful set
I1013 12:59:04.370184       1 shared_informer.go:320] Caches are synced for GC
I1013 12:59:04.372008       1 shared_informer.go:320] Caches are synced for deployment
I1013 12:59:04.373235       1 shared_informer.go:320] Caches are synced for job
I1013 12:59:04.373361       1 shared_informer.go:320] Caches are synced for cronjob
I1013 12:59:04.375630       1 shared_informer.go:320] Caches are synced for disruption
I1013 12:59:04.375671       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I1013 12:59:04.376754       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I1013 12:59:04.427721       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I1013 12:59:04.468519       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I1013 12:59:04.478104       1 shared_informer.go:320] Caches are synced for crt configmap
I1013 12:59:04.478119       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I1013 12:59:04.488996       1 shared_informer.go:320] Caches are synced for endpoint
I1013 12:59:04.493518       1 shared_informer.go:320] Caches are synced for endpoint_slice
I1013 12:59:04.563626       1 shared_informer.go:320] Caches are synced for HPA
I1013 12:59:04.570166       1 shared_informer.go:320] Caches are synced for resource quota
I1013 12:59:04.590022       1 shared_informer.go:320] Caches are synced for resource quota
I1013 12:59:04.673089       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="303.618673ms"
I1013 12:59:04.673354       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="76.841µs"
I1013 12:59:05.028987       1 shared_informer.go:320] Caches are synced for garbage collector
I1013 12:59:05.067532       1 shared_informer.go:320] Caches are synced for garbage collector
I1013 12:59:05.067569       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I1013 12:59:25.254230       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="5.467438ms"
I1013 12:59:25.254313       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="37.688µs"
I1013 12:59:29.362378       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="45.109µs"
I1013 12:59:30.899923       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="4.748142ms"
I1013 12:59:30.900061       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="91.541µs"
I1013 12:59:43.382400       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="6.672596ms"
I1013 12:59:43.382629       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="118.24µs"


==> kube-controller-manager [dfe228f82662] <==
I1013 12:34:43.215852       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I1013 12:34:43.216813       1 shared_informer.go:320] Caches are synced for node
I1013 12:34:43.216851       1 range_allocator.go:171] "Sending events to api server" logger="node-ipam-controller"
I1013 12:34:43.216864       1 range_allocator.go:177] "Starting range CIDR allocator" logger="node-ipam-controller"
I1013 12:34:43.216867       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I1013 12:34:43.216870       1 shared_informer.go:320] Caches are synced for cidrallocator
I1013 12:34:43.217029       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I1013 12:34:43.217057       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1013 12:34:43.218682       1 shared_informer.go:320] Caches are synced for attach detach
I1013 12:34:43.228555       1 range_allocator.go:422] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I1013 12:34:43.228682       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1013 12:34:43.228716       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1013 12:34:43.264219       1 shared_informer.go:320] Caches are synced for persistent volume
I1013 12:34:43.362355       1 shared_informer.go:320] Caches are synced for HPA
I1013 12:34:43.364292       1 shared_informer.go:320] Caches are synced for cronjob
I1013 12:34:43.365445       1 shared_informer.go:320] Caches are synced for TTL after finished
I1013 12:34:43.372433       1 shared_informer.go:320] Caches are synced for resource quota
I1013 12:34:43.414718       1 shared_informer.go:320] Caches are synced for job
I1013 12:34:43.433397       1 shared_informer.go:320] Caches are synced for resource quota
I1013 12:34:43.839035       1 shared_informer.go:320] Caches are synced for garbage collector
I1013 12:34:43.867384       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1013 12:34:43.872094       1 shared_informer.go:320] Caches are synced for garbage collector
I1013 12:34:43.872119       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I1013 12:34:44.324451       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="255.081188ms"
I1013 12:34:44.328992       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="4.464521ms"
I1013 12:34:44.329054       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="38.02µs"
I1013 12:34:44.329077       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="11.623µs"
I1013 12:34:44.332476       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="28.394µs"
I1013 12:34:45.338865       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="70.91µs"
I1013 12:34:49.688183       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1013 12:35:13.272812       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="13.908177ms"
I1013 12:35:13.272972       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="89.107µs"
I1013 12:38:37.758370       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="8.579627ms"
E1013 12:38:37.758441       1 replica_set.go:560] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4\" failed with pods \"dashboard-metrics-scraper-c5db448b4-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
I1013 12:38:37.762056       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="2.486403ms"
E1013 12:38:37.762112       1 replica_set.go:560] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4\" failed with pods \"dashboard-metrics-scraper-c5db448b4-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
I1013 12:38:37.765531       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="9.677541ms"
E1013 12:38:37.765567       1 replica_set.go:560] "Unhandled Error" err="sync \"kubernetes-dashboard/kubernetes-dashboard-695b96c756\" failed with pods \"kubernetes-dashboard-695b96c756-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
I1013 12:38:37.766988       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="2.389358ms"
E1013 12:38:37.767017       1 replica_set.go:560] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4\" failed with pods \"dashboard-metrics-scraper-c5db448b4-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
I1013 12:38:37.769604       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="2.810258ms"
E1013 12:38:37.769633       1 replica_set.go:560] "Unhandled Error" err="sync \"kubernetes-dashboard/kubernetes-dashboard-695b96c756\" failed with pods \"kubernetes-dashboard-695b96c756-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
I1013 12:38:37.800114       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="27.761415ms"
E1013 12:38:37.800168       1 replica_set.go:560] "Unhandled Error" err="sync \"kubernetes-dashboard/kubernetes-dashboard-695b96c756\" failed with pods \"kubernetes-dashboard-695b96c756-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
I1013 12:38:37.808101       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="8.142105ms"
I1013 12:38:37.812006       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="3.84287ms"
I1013 12:38:37.812087       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="31.204µs"
I1013 12:38:37.815730       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="24.648µs"
I1013 12:38:37.828775       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="7.217769ms"
I1013 12:38:37.832633       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="3.797094ms"
I1013 12:38:37.832789       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="58.978µs"
I1013 12:38:37.835706       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="28.745µs"
I1013 12:38:47.409426       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="4.151573ms"
I1013 12:38:47.409501       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="29.089µs"
I1013 12:39:06.494578       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="4.100995ms"
I1013 12:39:06.494651       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="34.1µs"
I1013 12:39:14.109057       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1013 12:44:20.363503       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1013 12:49:26.284021       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1013 12:54:32.431979       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [5a02c15bb8f1] <==
E1013 12:34:45.117621       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E1013 12:34:45.122185       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I1013 12:34:45.130064       1 server_linux.go:66] "Using iptables proxy"
I1013 12:34:45.476155       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.220.2"]
E1013 12:34:45.476253       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1013 12:34:45.492641       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1013 12:34:45.492716       1 server_linux.go:169] "Using iptables Proxier"
I1013 12:34:45.494088       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E1013 12:34:45.497546       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E1013 12:34:45.501420       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I1013 12:34:45.501546       1 server.go:483] "Version info" version="v1.31.0"
I1013 12:34:45.501573       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1013 12:34:45.502573       1 config.go:197] "Starting service config controller"
I1013 12:34:45.502618       1 config.go:326] "Starting node config controller"
I1013 12:34:45.502661       1 shared_informer.go:313] Waiting for caches to sync for node config
I1013 12:34:45.502660       1 shared_informer.go:313] Waiting for caches to sync for service config
I1013 12:34:45.502752       1 config.go:104] "Starting endpoint slice config controller"
I1013 12:34:45.502769       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1013 12:34:45.602799       1 shared_informer.go:320] Caches are synced for node config
I1013 12:34:45.602847       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1013 12:34:45.604000       1 shared_informer.go:320] Caches are synced for service config


==> kube-proxy [95c999d899d1] <==
E1013 12:59:03.591546       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E1013 12:59:03.658918       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I1013 12:59:03.674167       1 server_linux.go:66] "Using iptables proxy"
I1013 12:59:04.194036       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.220.2"]
E1013 12:59:04.194101       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1013 12:59:04.213689       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1013 12:59:04.213751       1 server_linux.go:169] "Using iptables Proxier"
I1013 12:59:04.215313       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E1013 12:59:04.219115       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E1013 12:59:04.222777       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I1013 12:59:04.222861       1 server.go:483] "Version info" version="v1.31.0"
I1013 12:59:04.222889       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1013 12:59:04.223812       1 config.go:104] "Starting endpoint slice config controller"
I1013 12:59:04.223837       1 config.go:326] "Starting node config controller"
I1013 12:59:04.223845       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1013 12:59:04.223847       1 shared_informer.go:313] Waiting for caches to sync for node config
I1013 12:59:04.223812       1 config.go:197] "Starting service config controller"
I1013 12:59:04.223875       1 shared_informer.go:313] Waiting for caches to sync for service config
I1013 12:59:04.324310       1 shared_informer.go:320] Caches are synced for node config
I1013 12:59:04.324405       1 shared_informer.go:320] Caches are synced for service config
I1013 12:59:04.324420       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [1fe4186aab2f] <==
I1013 12:58:59.295241       1 serving.go:386] Generated self-signed cert in-memory
I1013 12:59:01.263540       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I1013 12:59:01.263688       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1013 12:59:01.270941       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1013 12:59:01.271160       1 requestheader_controller.go:172] Starting RequestHeaderAuthRequestController
I1013 12:59:01.271207       1 shared_informer.go:313] Waiting for caches to sync for RequestHeaderAuthRequestController
I1013 12:59:01.271257       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1013 12:59:01.274297       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1013 12:59:01.274330       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1013 12:59:01.274351       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I1013 12:59:01.274778       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I1013 12:59:01.371319       1 shared_informer.go:320] Caches are synced for RequestHeaderAuthRequestController
I1013 12:59:01.374609       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1013 12:59:01.375004       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file


==> kube-scheduler [f16361076e65] <==
I1013 12:34:35.133742       1 serving.go:386] Generated self-signed cert in-memory
W1013 12:34:36.456063       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1013 12:34:36.456118       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1013 12:34:36.456129       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W1013 12:34:36.456136       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1013 12:34:36.547101       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I1013 12:34:36.547154       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1013 12:34:36.548817       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1013 12:34:36.548880       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1013 12:34:36.549018       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1013 12:34:36.549116       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
W1013 12:34:36.550272       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1013 12:34:36.550336       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1013 12:34:36.550284       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1013 12:34:36.550392       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W1013 12:34:36.551082       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1013 12:34:36.551113       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1013 12:34:36.551125       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1013 12:34:36.551137       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1013 12:34:36.551142       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
E1013 12:34:36.551160       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W1013 12:34:36.551188       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1013 12:34:36.551199       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1013 12:34:36.551207       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1013 12:34:36.551213       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
E1013 12:34:36.551208       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1013 12:34:36.551233       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1013 12:34:36.551231       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1013 12:34:36.551234       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1013 12:34:36.551238       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1013 12:34:36.551241       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1013 12:34:36.551247       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W1013 12:34:36.551254       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1013 12:34:36.551252       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
E1013 12:34:36.551257       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1013 12:34:36.551260       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1013 12:34:36.551266       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
E1013 12:34:36.551276       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
E1013 12:34:36.551256       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W1013 12:34:36.551346       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1013 12:34:36.551370       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1013 12:34:37.394470       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1013 12:34:37.394532       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1013 12:34:37.394476       1 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1013 12:34:37.394567       1 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W1013 12:34:37.680799       1 reflector.go:561] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1013 12:34:37.680857       1 reflector.go:158] "Unhandled Error" err="runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
I1013 12:34:40.349417       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1013 12:58:36.863661       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I1013 12:58:36.863706       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
E1013 12:58:36.864054       1 run.go:72] "command failed" err="finished without leader elect"
I1013 12:58:36.863889       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Oct 13 12:58:57 minikube kubelet[1642]: I1013 12:58:57.207068    1642 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/f32ad0ab2974a62a0459b2603686f8e6-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"f32ad0ab2974a62a0459b2603686f8e6\") " pod="kube-system/kube-apiserver-minikube"
Oct 13 12:58:57 minikube kubelet[1642]: I1013 12:58:57.207088    1642 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/f32ad0ab2974a62a0459b2603686f8e6-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"f32ad0ab2974a62a0459b2603686f8e6\") " pod="kube-system/kube-apiserver-minikube"
Oct 13 12:58:57 minikube kubelet[1642]: I1013 12:58:57.207098    1642 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/40f5f661ab65f2e4bfe41ac2993c01de-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"40f5f661ab65f2e4bfe41ac2993c01de\") " pod="kube-system/kube-controller-manager-minikube"
Oct 13 12:58:57 minikube kubelet[1642]: I1013 12:58:57.207120    1642 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/40f5f661ab65f2e4bfe41ac2993c01de-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"40f5f661ab65f2e4bfe41ac2993c01de\") " pod="kube-system/kube-controller-manager-minikube"
Oct 13 12:58:57 minikube kubelet[1642]: I1013 12:58:57.207136    1642 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/e039200acb850c82bb901653cc38ff6e-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"e039200acb850c82bb901653cc38ff6e\") " pod="kube-system/kube-scheduler-minikube"
Oct 13 12:58:57 minikube kubelet[1642]: I1013 12:58:57.207227    1642 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/40f5f661ab65f2e4bfe41ac2993c01de-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"40f5f661ab65f2e4bfe41ac2993c01de\") " pod="kube-system/kube-controller-manager-minikube"
Oct 13 12:58:57 minikube kubelet[1642]: I1013 12:58:57.207284    1642 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/521651f224f5a7754c89ccaa18ca6ac8-etcd-certs\") pod \"etcd-minikube\" (UID: \"521651f224f5a7754c89ccaa18ca6ac8\") " pod="kube-system/etcd-minikube"
Oct 13 12:58:57 minikube kubelet[1642]: I1013 12:58:57.207312    1642 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/f32ad0ab2974a62a0459b2603686f8e6-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"f32ad0ab2974a62a0459b2603686f8e6\") " pod="kube-system/kube-apiserver-minikube"
Oct 13 12:58:57 minikube kubelet[1642]: I1013 12:58:57.207327    1642 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/f32ad0ab2974a62a0459b2603686f8e6-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"f32ad0ab2974a62a0459b2603686f8e6\") " pod="kube-system/kube-apiserver-minikube"
Oct 13 12:58:57 minikube kubelet[1642]: I1013 12:58:57.207338    1642 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/40f5f661ab65f2e4bfe41ac2993c01de-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"40f5f661ab65f2e4bfe41ac2993c01de\") " pod="kube-system/kube-controller-manager-minikube"
Oct 13 12:58:57 minikube kubelet[1642]: I1013 12:58:57.207375    1642 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/40f5f661ab65f2e4bfe41ac2993c01de-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"40f5f661ab65f2e4bfe41ac2993c01de\") " pod="kube-system/kube-controller-manager-minikube"
Oct 13 12:58:57 minikube kubelet[1642]: I1013 12:58:57.376505    1642 kubelet_node_status.go:72] "Attempting to register node" node="minikube"
Oct 13 12:58:57 minikube kubelet[1642]: E1013 12:58:57.376896    1642 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.220.2:8443: connect: connection refused" node="minikube"
Oct 13 12:58:57 minikube kubelet[1642]: E1013 12:58:57.596737    1642 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.220.2:8443: connect: connection refused" interval="800ms"
Oct 13 12:58:57 minikube kubelet[1642]: I1013 12:58:57.778007    1642 kubelet_node_status.go:72] "Attempting to register node" node="minikube"
Oct 13 12:58:57 minikube kubelet[1642]: E1013 12:58:57.778409    1642 kubelet_node_status.go:95] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.220.2:8443: connect: connection refused" node="minikube"
Oct 13 12:58:57 minikube kubelet[1642]: W1013 12:58:57.797866    1642 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0": dial tcp 192.168.220.2:8443: connect: connection refused
Oct 13 12:58:57 minikube kubelet[1642]: E1013 12:58:57.798115    1642 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get \"https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0\": dial tcp 192.168.220.2:8443: connect: connection refused" logger="UnhandledError"
Oct 13 12:58:57 minikube kubelet[1642]: W1013 12:58:57.960726    1642 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.RuntimeClass: Get "https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 192.168.220.2:8443: connect: connection refused
Oct 13 12:58:57 minikube kubelet[1642]: E1013 12:58:57.960822    1642 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get \"https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0\": dial tcp 192.168.220.2:8443: connect: connection refused" logger="UnhandledError"
Oct 13 12:58:58 minikube kubelet[1642]: W1013 12:58:58.360863    1642 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://control-plane.minikube.internal:8443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0": dial tcp 192.168.220.2:8443: connect: connection refused
Oct 13 12:58:58 minikube kubelet[1642]: E1013 12:58:58.360977    1642 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"https://control-plane.minikube.internal:8443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0\": dial tcp 192.168.220.2:8443: connect: connection refused" logger="UnhandledError"
Oct 13 12:58:58 minikube kubelet[1642]: E1013 12:58:58.460589    1642 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.220.2:8443: connect: connection refused" interval="1.6s"
Oct 13 12:58:58 minikube kubelet[1642]: W1013 12:58:58.460593    1642 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.220.2:8443: connect: connection refused
Oct 13 12:58:58 minikube kubelet[1642]: E1013 12:58:58.460687    1642 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get \"https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.220.2:8443: connect: connection refused" logger="UnhandledError"
Oct 13 12:58:58 minikube kubelet[1642]: I1013 12:58:58.580631    1642 kubelet_node_status.go:72] "Attempting to register node" node="minikube"
Oct 13 12:59:01 minikube kubelet[1642]: I1013 12:59:01.272655    1642 kubelet_node_status.go:111] "Node was previously registered" node="minikube"
Oct 13 12:59:01 minikube kubelet[1642]: I1013 12:59:01.273036    1642 kubelet_node_status.go:75] "Successfully registered node" node="minikube"
Oct 13 12:59:01 minikube kubelet[1642]: I1013 12:59:01.273093    1642 kuberuntime_manager.go:1633] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Oct 13 12:59:01 minikube kubelet[1642]: I1013 12:59:01.274190    1642 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Oct 13 12:59:01 minikube kubelet[1642]: E1013 12:59:01.667355    1642 kubelet.go:1915] "Failed creating a mirror pod for" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Oct 13 12:59:02 minikube kubelet[1642]: I1013 12:59:02.060321    1642 apiserver.go:52] "Watching apiserver"
Oct 13 12:59:02 minikube kubelet[1642]: I1013 12:59:02.095968    1642 desired_state_of_world_populator.go:154] "Finished populating initial desired state of world"
Oct 13 12:59:02 minikube kubelet[1642]: I1013 12:59:02.169826    1642 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/7506a22d-84f1-44e1-bb20-85f38cada47a-lib-modules\") pod \"kube-proxy-ckhr5\" (UID: \"7506a22d-84f1-44e1-bb20-85f38cada47a\") " pod="kube-system/kube-proxy-ckhr5"
Oct 13 12:59:02 minikube kubelet[1642]: I1013 12:59:02.169905    1642 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/7506a22d-84f1-44e1-bb20-85f38cada47a-xtables-lock\") pod \"kube-proxy-ckhr5\" (UID: \"7506a22d-84f1-44e1-bb20-85f38cada47a\") " pod="kube-system/kube-proxy-ckhr5"
Oct 13 12:59:02 minikube kubelet[1642]: I1013 12:59:02.170039    1642 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/59ad433d-7158-4c16-8778-38ca462bc0cf-tmp\") pod \"storage-provisioner\" (UID: \"59ad433d-7158-4c16-8778-38ca462bc0cf\") " pod="kube-system/storage-provisioner"
Oct 13 12:59:02 minikube kubelet[1642]: I1013 12:59:02.962790    1642 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="44be68b2e2cfb4120c41eb4e527c499ca495c34053404801bc486934b3239215"
Oct 13 12:59:02 minikube kubelet[1642]: I1013 12:59:02.980254    1642 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="48b868f07e3e44141c28d80947172ebe5579b017cb3fa46fc10f8513eba3f253"
Oct 13 12:59:07 minikube kubelet[1642]: E1013 12:59:07.145860    1642 summary_sys_containers.go:51] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Oct 13 12:59:07 minikube kubelet[1642]: E1013 12:59:07.145907    1642 helpers.go:854] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Oct 13 12:59:10 minikube kubelet[1642]: I1013 12:59:10.885457    1642 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Oct 13 12:59:17 minikube kubelet[1642]: E1013 12:59:17.157765    1642 summary_sys_containers.go:51] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Oct 13 12:59:17 minikube kubelet[1642]: E1013 12:59:17.157810    1642 helpers.go:854] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Oct 13 12:59:25 minikube kubelet[1642]: I1013 12:59:25.221964    1642 scope.go:117] "RemoveContainer" containerID="fe0428a3dbd063474260a6e5e79cc403d54456cbf207ee16552deea3a3c58ce9"
Oct 13 12:59:25 minikube kubelet[1642]: I1013 12:59:25.222279    1642 scope.go:117] "RemoveContainer" containerID="ecc4a7ba70342c4f5428398d951b71d958d6121b8a039edc6d275145ad5cdebd"
Oct 13 12:59:25 minikube kubelet[1642]: E1013 12:59:25.222436    1642 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with CrashLoopBackOff: \"back-off 10s restarting failed container=kubernetes-dashboard pod=kubernetes-dashboard-695b96c756-mq464_kubernetes-dashboard(69da51f5-2429-40c4-920b-1d6a3b0ec446)\"" pod="kubernetes-dashboard/kubernetes-dashboard-695b96c756-mq464" podUID="69da51f5-2429-40c4-920b-1d6a3b0ec446"
Oct 13 12:59:25 minikube kubelet[1642]: I1013 12:59:25.227733    1642 scope.go:117] "RemoveContainer" containerID="675e7866f0690ba25c58273ba0ec3ba34fe8e9e80c45c05dfc5a3d29fa7641da"
Oct 13 12:59:25 minikube kubelet[1642]: E1013 12:59:25.227853    1642 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(59ad433d-7158-4c16-8778-38ca462bc0cf)\"" pod="kube-system/storage-provisioner" podUID="59ad433d-7158-4c16-8778-38ca462bc0cf"
Oct 13 12:59:25 minikube kubelet[1642]: I1013 12:59:25.368359    1642 scope.go:117] "RemoveContainer" containerID="4bda03161be2f6c087fab960d1c4fcf08247d24b5c912a64059ce2c47daa0221"
Oct 13 12:59:27 minikube kubelet[1642]: E1013 12:59:27.172241    1642 summary_sys_containers.go:51] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Oct 13 12:59:27 minikube kubelet[1642]: E1013 12:59:27.172285    1642 helpers.go:854] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Oct 13 12:59:29 minikube kubelet[1642]: I1013 12:59:29.350337    1642 scope.go:117] "RemoveContainer" containerID="ecc4a7ba70342c4f5428398d951b71d958d6121b8a039edc6d275145ad5cdebd"
Oct 13 12:59:29 minikube kubelet[1642]: E1013 12:59:29.350495    1642 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kubernetes-dashboard\" with CrashLoopBackOff: \"back-off 10s restarting failed container=kubernetes-dashboard pod=kubernetes-dashboard-695b96c756-mq464_kubernetes-dashboard(69da51f5-2429-40c4-920b-1d6a3b0ec446)\"" pod="kubernetes-dashboard/kubernetes-dashboard-695b96c756-mq464" podUID="69da51f5-2429-40c4-920b-1d6a3b0ec446"
Oct 13 12:59:36 minikube kubelet[1642]: I1013 12:59:36.004444    1642 scope.go:117] "RemoveContainer" containerID="675e7866f0690ba25c58273ba0ec3ba34fe8e9e80c45c05dfc5a3d29fa7641da"
Oct 13 12:59:37 minikube kubelet[1642]: E1013 12:59:37.183766    1642 summary_sys_containers.go:51] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Oct 13 12:59:37 minikube kubelet[1642]: E1013 12:59:37.183804    1642 helpers.go:854] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Oct 13 12:59:43 minikube kubelet[1642]: I1013 12:59:43.004837    1642 scope.go:117] "RemoveContainer" containerID="ecc4a7ba70342c4f5428398d951b71d958d6121b8a039edc6d275145ad5cdebd"
Oct 13 12:59:47 minikube kubelet[1642]: E1013 12:59:47.198812    1642 summary_sys_containers.go:51] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Oct 13 12:59:47 minikube kubelet[1642]: E1013 12:59:47.198855    1642 helpers.go:854] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Oct 13 12:59:57 minikube kubelet[1642]: E1013 12:59:57.219485    1642 cadvisor_stats_provider.go:516] "Partial failure issuing cadvisor.ContainerInfoV2" err="partial failures: [\"/kubepods/burstable/pod40f5f661ab65f2e4bfe41ac2993c01de/4dd51d4673c4faaacef1641ad7b5b7d88ce53c7843d3c21fb5b3e4deaa57158c\": RecentStats: unable to find data in memory cache]"


==> kubernetes-dashboard [7d2430ff5926] <==
2024/10/13 12:59:43 Starting overwatch
2024/10/13 12:59:43 Using namespace: kubernetes-dashboard
2024/10/13 12:59:43 Using in-cluster config to connect to apiserver
2024/10/13 12:59:43 Using secret token for csrf signing
2024/10/13 12:59:43 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/10/13 12:59:43 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2024/10/13 12:59:43 Successful initial request to the apiserver, version: v1.31.0
2024/10/13 12:59:43 Generating JWE encryption key
2024/10/13 12:59:43 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2024/10/13 12:59:43 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/10/13 12:59:43 Initializing JWE encryption key from synchronized object
2024/10/13 12:59:43 Creating in-cluster Sidecar client
2024/10/13 12:59:43 Serving insecurely on HTTP port: 9090
2024/10/13 12:59:43 Successful request to sidecar


==> kubernetes-dashboard [ecc4a7ba7034] <==
2024/10/13 12:59:03 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": dial tcp 10.96.0.1:443: connect: connection refused

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0xc00075fae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x30e
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0xc00048a580)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x94
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x19aba3a?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x32
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1cf
2024/10/13 12:59:03 Using namespace: kubernetes-dashboard
2024/10/13 12:59:03 Using in-cluster config to connect to apiserver
2024/10/13 12:59:03 Using secret token for csrf signing
2024/10/13 12:59:03 Initializing csrf token from kubernetes-dashboard-csrf secret


==> storage-provisioner [1f01d631846d] <==
I1013 12:59:36.151684       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1013 12:59:36.157636       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1013 12:59:36.157680       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1013 12:59:53.554265       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1013 12:59:53.554312       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"d3fa9539-935b-420c-a031-764815f911fb", APIVersion:"v1", ResourceVersion:"1749", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_04ff2a91-bd76-4b37-8477-d8cea2d62402 became leader
I1013 12:59:53.554348       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_04ff2a91-bd76-4b37-8477-d8cea2d62402!
I1013 12:59:53.654555       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_04ff2a91-bd76-4b37-8477-d8cea2d62402!


==> storage-provisioner [675e7866f069] <==
I1013 12:59:03.488776       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1013 12:59:24.539935       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

